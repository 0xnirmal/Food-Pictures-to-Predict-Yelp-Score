{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !conda install --yes --prefix {sys.prefix} scikit-image\n",
    "# !pip3 install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data\n",
    "from skimage import io, transform\n",
    "from skimage.measure import block_reduce\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='Deep Learning JHU Assignment 1 - Fashion-MNIST')\n",
    "parser.add_argument('--batch-size', type=int, default=256, metavar='B',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='TB',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='E',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--optimizer', type=str, default='sgd', metavar='O',\n",
    "                    help='Optimizer options are sgd, p1sgd, adam, rms_prop')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='MO',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log_interval', type=int, default=100, metavar='I',\n",
    "                    help=\"\"\"how many batches to wait before logging detailed\n",
    "                            training status, 0 means never log \"\"\")\n",
    "parser.add_argument('--dataset', type=str, default='mnist', metavar='D',\n",
    "                    help='Options are mnist and fashion_mnist')\n",
    "parser.add_argument('--data_dir', type=str, default='../data/', metavar='F',\n",
    "                    help='Where to put data')\n",
    "parser.add_argument('--name', type=str, default='', metavar='N',\n",
    "                    help=\"\"\"A name for this training run, this\n",
    "                            affects the directory so use underscores and not spaces.\"\"\")\n",
    "parser.add_argument('--model', type=str, default='default', metavar='M',\n",
    "                    help=\"\"\"Options are default, P2Q7DefaultChannelsNet,\n",
    "                    P2Q7HalfChannelsNet, P2Q7DoubleChannelsNet,\n",
    "                    P2Q8BatchNormNet, P2Q9DropoutNet, P2Q10DropoutBatchnormNet,\n",
    "                    P2Q11ExtraConvNet, P2Q12RemoveLayerNet, and P2Q13UltimateNet.\"\"\")\n",
    "parser.add_argument('--print_log', action='store_true', default=False,\n",
    "                    help='prints the csv log when training is complete')\n",
    "parser.add_argument('--dropout_rate', type=float, default=0.5)\n",
    "parser.add_argument(\"-f\", \"--jupyter-json\")\n",
    "\n",
    "\n",
    "required = object()\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_photo_file(photo_name):\n",
    "    return \"data/yelp_photos/photos/\" + photo_name + \".jpg\"\n",
    "\n",
    "def resize_img(img):\n",
    "    return transform.resize(img, (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class YelpDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, batch_size, shuffle=True):\n",
    "        self.df = df \n",
    "        self.batch_size = batch_size\n",
    "        if shuffle:\n",
    "            self.df = self.df.sample(frac=1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        photo_names = self.df.iloc[idx:idx+self.batch_size].index.tolist()\n",
    "        file_names = [get_photo_file(photo_name) for photo_name in photo_names]\n",
    "        images = [io.imread(file_name) for file_name in file_names]\n",
    "        images = [resize_img(img) for img in images] \n",
    "        label = self.df.iloc[idx:idx+self.batch_size].label.tolist()\n",
    "        return (torch.Tensor(images),torch.Tensor(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biz_df = pd.read_csv(\"data/clean_business.csv\").set_index(\"business_id\")\n",
    "photo_df = pd.read_csv(\"data/clean_photo.csv\").set_index(\"photo_id\")\n",
    "df = photo_df.copy(deep=True)\n",
    "df[\"label\"] = pd.Series(biz_df.loc[df[\"business_id\"]][\"stars\"]).tolist()\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# train_df = df.iloc[0:int(len(df) * 0.7)]\n",
    "# val_df = df.iloc[int(len(df) * 0.7):]\n",
    "\n",
    "val_df = df.iloc[0:1000]\n",
    "train_df = df.iloc[1000:11000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = YelpDataset(train_df, batch_size=args.batch_size)\n",
    "val_loader = YelpDataset(val_df, batch_size=len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/python3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "val_img, val_label = val_loader[0]\n",
    "val_img = Variable(val_img)\n",
    "val_labeel = Variable(val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(BasicNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 5, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(5, 10, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(28090, 100)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 28090)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = BasicNet()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_step(step):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    input_batch, label_batch = train_loader[step]\n",
    "    input_batch = Variable(input_batch)\n",
    "    label_batch = Variable(label_batch)\n",
    "    output_batch = model(input_batch)\n",
    "\n",
    "    loss = F.l1_loss(output_batch, label_batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "    return loss.data[0]\n",
    "\n",
    "def train_epoch():\n",
    "    \n",
    "    model.train()\n",
    "    i = 0\n",
    "    loss_list = []\n",
    "    for input_batch, label_batch in train_loader:\n",
    "    \n",
    "        input_batch = Variable(input_batch)\n",
    "        label_batch = Variable(label_batch)\n",
    "        output_batch = model(input_batch)\n",
    "\n",
    "        loss = F.l1_loss(output_batch, label_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Training step \" + str(i) + \" \" + str(loss.data[0]))\n",
    "        i += 1\n",
    "        loss_list.append(loss)\n",
    "        if i == 10:\n",
    "            break\n",
    "    \n",
    "    total_loss = 0\n",
    "    for loss in loss_list:\n",
    "        total_loss += loss\n",
    "    total_loss /= i\n",
    "\n",
    "    return total_loss, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/python3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step 0 3.571491241455078\n",
      "Training step 1 3.5412356853485107\n",
      "Training step 2 3.57373309135437\n",
      "Training step 3 3.5447933673858643\n",
      "Training step 4 3.472736358642578\n",
      "Training step 5 3.4579105377197266\n",
      "Training step 6 3.478287935256958\n",
      "Training step 7 3.4323363304138184\n",
      "Training step 8 3.334973096847534\n",
      "Training step 9 3.239255428314209\n"
     ]
    }
   ],
   "source": [
    "for module in model.children():\n",
    "    module.reset_parameters()\n",
    "    \n",
    "avg_loss, loss_list = train_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3.4647\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for module in model.children():\n",
    "#     module.reset_parameters()\n",
    "\n",
    "# info = []\n",
    "# num_steps = 100\n",
    "# for step in range(num_steps):\n",
    "#     train_loss = train_step(step)\n",
    "#     print(train_loss)\n",
    "#     info.append(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(range(num_steps), info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# pred = model(val_img[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pred - Variable(val_label[0:30].view(30, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !conda install --yes --prefix {sys.prefix} scikit-image\n",
    "# !pip3 install scikit-image\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data\n",
    "from skimage import io, transform\n",
    "from skimage.measure import block_reduce\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Deep Learning JHU Assignment 1 - Fashion-MNIST')\n",
    "parser.add_argument('--batch-size', type=int, default=256, metavar='B',\n",
    "\t\t\t\t\thelp='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='TB',\n",
    "\t\t\t\t\thelp='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='E',\n",
    "\t\t\t\t\thelp='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "\t\t\t\t\thelp='learning rate (default: 0.01)')\n",
    "parser.add_argument('--optimizer', type=str, default='sgd', metavar='O',\n",
    "\t\t\t\t\thelp='Optimizer options are sgd, p1sgd, adam, rms_prop')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='MO',\n",
    "\t\t\t\t\thelp='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "\t\t\t\t\thelp='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "\t\t\t\t\thelp='random seed (default: 1)')\n",
    "parser.add_argument('--log_interval', type=int, default=100, metavar='I',\n",
    "\t\t\t\t\thelp=\"\"\"how many batches to wait before logging detailed\n",
    "\t\t\t\t\t\t\ttraining status, 0 means never log \"\"\")\n",
    "parser.add_argument('--dataset', type=str, default='mnist', metavar='D',\n",
    "\t\t\t\t\thelp='Options are mnist and fashion_mnist')\n",
    "parser.add_argument('--data_dir', type=str, default='data/', metavar='F',\n",
    "\t\t\t\t\thelp='Where to put data')\n",
    "parser.add_argument('--photos_dir', type=str, default='data/yelp_photos/photos/', metavar='F',\n",
    "\t\t\t\t\thelp='Where to put photos')\n",
    "parser.add_argument('--name', type=str, default='', metavar='N',\n",
    "\t\t\t\t\thelp=\"\"\"A name for this training run, this\n",
    "\t\t\t\t\t\t\taffects the directory so use underscores and not spaces.\"\"\")\n",
    "parser.add_argument('--model', type=str, default='default', metavar='M',\n",
    "\t\t\t\t\thelp=\"\"\"Options are default, P2Q7DefaultChannelsNet,\n",
    "\t\t\t\t\tP2Q7HalfChannelsNet, P2Q7DoubleChannelsNet,\n",
    "\t\t\t\t\tP2Q8BatchNormNet, P2Q9DropoutNet, P2Q10DropoutBatchnormNet,\n",
    "\t\t\t\t\tP2Q11ExtraConvNet, P2Q12RemoveLayerNet, and P2Q13UltimateNet.\"\"\")\n",
    "parser.add_argument('--print_log', action='store_true', default=False,\n",
    "\t\t\t\t\thelp='prints the csv log when training is complete')\n",
    "parser.add_argument('--dropout_rate', type=float, default=0.5)\n",
    "parser.add_argument(\"-f\", \"--jupyter-json\")\n",
    "parser.add_argument('--loss', type=str, default='l1', metavar='LOSS',\n",
    "\t\t\t\t\thelp='l1, mse')\n",
    "\n",
    "\n",
    "\n",
    "required = object()\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def get_photo_file(photo_name):\n",
    "\treturn args.photos_dir + photo_name + \".jpg\"\n",
    "\n",
    "def resize_img(img):\n",
    "\treturn transform.resize(img, (224, 224))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YelpDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, df):\n",
    "\t\tself.df = df \n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.df)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tphoto_name = self.df.iloc[idx].name\n",
    "\t\tfile_name = get_photo_file(photo_name)\n",
    "\t\timg = io.imread(file_name)\n",
    "\t\timg = resize_img(img) \n",
    "\t\tlabel = self.df.iloc[idx].label\n",
    "\t\treturn (torch.Tensor(img), torch.Tensor([label]))\n",
    "\n",
    "biz_df = pd.read_csv(args.data_dir + \"clean_business.csv\").set_index(\"business_id\")\n",
    "photo_df = pd.read_csv(args.data_dir + \"clean_photo.csv\").set_index(\"photo_id\")\n",
    "df = photo_df.copy(deep=True)\n",
    "df[\"label\"] = pd.Series(biz_df.loc[df[\"business_id\"]][\"stars\"]).tolist()\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# train_df = df.iloc[0:int(len(df) * 0.7)]\n",
    "# val_df = df.iloc[int(len(df) * 0.7):]\n",
    "\n",
    "val_df = df.iloc[0:1000]\n",
    "train_df = df.iloc[1000:11000]\n",
    "\n",
    "\n",
    "train_loader = YelpDataset(train_df)\n",
    "val_loader = YelpDataset(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/python3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\n",
       " ( 0 ,.,.) = \n",
       "   0.1647  0.2314  0.2706\n",
       "   0.1647  0.2314  0.2706\n",
       "   0.1647  0.2314  0.2706\n",
       "            ⋮            \n",
       "   0.3098  0.3765  0.4039\n",
       "   0.3098  0.3765  0.4039\n",
       "   0.3098  0.3765  0.4039\n",
       " \n",
       " ( 1 ,.,.) = \n",
       "   0.1647  0.2314  0.2706\n",
       "   0.1647  0.2314  0.2706\n",
       "   0.1647  0.2314  0.2706\n",
       "            ⋮            \n",
       "   0.3105  0.3771  0.4046\n",
       "   0.3105  0.3771  0.4046\n",
       "   0.3105  0.3771  0.4046\n",
       " \n",
       " ( 2 ,.,.) = \n",
       "   0.1647  0.2314  0.2706\n",
       "   0.1647  0.2314  0.2706\n",
       "   0.1647  0.2314  0.2706\n",
       "            ⋮            \n",
       "   0.3137  0.3804  0.4078\n",
       "   0.3137  0.3804  0.4078\n",
       "   0.3137  0.3804  0.4078\n",
       " ... \n",
       " \n",
       " (221,.,.) = \n",
       "   0.0390  0.0743  0.1018\n",
       "   0.0390  0.0743  0.1018\n",
       "   0.0390  0.0743  0.1018\n",
       "            ⋮            \n",
       "   0.0275  0.0706  0.0941\n",
       "   0.0314  0.0745  0.0980\n",
       "   0.0353  0.0784  0.1020\n",
       " \n",
       " (222,.,.) = \n",
       "   0.0353  0.0706  0.0980\n",
       "   0.0353  0.0706  0.0980\n",
       "   0.0353  0.0706  0.0980\n",
       "            ⋮            \n",
       "   0.0275  0.0706  0.0941\n",
       "   0.0314  0.0745  0.0980\n",
       "   0.0353  0.0784  0.1020\n",
       " \n",
       " (223,.,.) = \n",
       "   0.0329  0.0682  0.0956\n",
       "   0.0329  0.0682  0.0956\n",
       "   0.0329  0.0682  0.0956\n",
       "            ⋮            \n",
       "   0.0275  0.0706  0.0941\n",
       "   0.0314  0.0745  0.0980\n",
       "   0.0353  0.0784  0.1020\n",
       " [torch.FloatTensor of size 224x224x3], \n",
       "  3.5000\n",
       " [torch.FloatTensor of size 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/python3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "photo_name = df.iloc[0].name\n",
    "file_name = get_photo_file(photo_name)\n",
    "img = io.imread(file_name)\n",
    "img = resize_img(img) \n",
    "label = df.iloc[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 ,.,.) = \n",
       "  0.0980  0.0431  0.0078\n",
       "  0.0980  0.0431  0.0078\n",
       "  0.0980  0.0431  0.0078\n",
       "           ⋮            \n",
       "  0.5098  0.4392  0.3608\n",
       "  0.5059  0.4353  0.3569\n",
       "  0.5020  0.4314  0.3529\n",
       "\n",
       "( 1 ,.,.) = \n",
       "  0.0980  0.0431  0.0078\n",
       "  0.0980  0.0431  0.0078\n",
       "  0.0980  0.0431  0.0078\n",
       "           ⋮            \n",
       "  0.5098  0.4385  0.3622\n",
       "  0.5059  0.4346  0.3583\n",
       "  0.5020  0.4307  0.3543\n",
       "\n",
       "( 2 ,.,.) = \n",
       "  0.0980  0.0431  0.0078\n",
       "  0.0980  0.0431  0.0078\n",
       "  0.0980  0.0431  0.0078\n",
       "           ⋮            \n",
       "  0.5098  0.4353  0.3686\n",
       "  0.5059  0.4314  0.3647\n",
       "  0.5020  0.4275  0.3608\n",
       "... \n",
       "\n",
       "(221,.,.) = \n",
       "  0.0235  0.0078  0.0039\n",
       "  0.0235  0.0078  0.0039\n",
       "  0.0235  0.0078  0.0039\n",
       "           ⋮            \n",
       "  0.4704  0.4038  0.3646\n",
       "  0.4704  0.4038  0.3646\n",
       "  0.4704  0.4038  0.3646\n",
       "\n",
       "(222,.,.) = \n",
       "  0.0235  0.0078  0.0039\n",
       "  0.0235  0.0078  0.0039\n",
       "  0.0235  0.0078  0.0039\n",
       "           ⋮            \n",
       "  0.4634  0.3968  0.3576\n",
       "  0.4634  0.3968  0.3576\n",
       "  0.4634  0.3968  0.3576\n",
       "\n",
       "(223,.,.) = \n",
       "  0.0235  0.0078  0.0039\n",
       "  0.0235  0.0078  0.0039\n",
       "  0.0235  0.0078  0.0039\n",
       "           ⋮            \n",
       "  0.4604  0.3961  0.3497\n",
       "  0.4604  0.3961  0.3497\n",
       "  0.4604  0.3961  0.3497\n",
       "[torch.FloatTensor of size 224x224x3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 3\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
